from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from llama_cpp import Llama
import uvicorn
import json
import time
import threading
import asyncio
import gc
import os
import psutil

app = FastAPI(title="Gemma Chat API", version="1.0.0")


llm = None
last_used = 0
IDLE_TIMEOUT = 300  #è¨­ç‚º 300 ç§’


def get_llm():
    global llm, last_used
    if llm is None:
        try:
            llm = Llama(
                model_path="/aihome/llama.cpp/models/Gemma-3-TAIDE-12b-Chat-Q6_K.gguf",
                n_ctx=4096,
                n_threads=os.cpu_count(),      # âœ… CPU è‡ªå‹•ä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒ
                n_gpu_layers=40                # âœ… GPU æ‰‹å‹•æŒ‡å®šå±¤æ•¸ï¼ˆ12GB å»ºè­° 40~50ï¼‰
            )
            print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise RuntimeError("æ¨¡å‹åˆå§‹åŒ–å¤±æ•—")
    last_used = time.time()
    return llm


def idle_monitor():
    global llm, last_used
    while True:
        if llm and time.time() - last_used > IDLE_TIMEOUT:
            print("ğŸ§¹ æ¨¡å‹é–’ç½®é‡‹æ”¾ä¸­")
            try:
                # å˜—è©¦å–å¾—é€²ç¨‹ PID ä¸¦çµ‚æ­¢ï¼ˆé¸ç”¨ï¼‰
                for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                    if proc.info['cmdline'] and "llama.cpp" in " ".join(proc.info['cmdline']):
                        print(f"ğŸ”ª çµ‚æ­¢æ¨¡å‹é€²ç¨‹ PID {proc.info['pid']}")
                        proc.terminate()
                llm = None
                gc.collect()
                print("âœ… æ¨¡å‹å·²é‡‹æ”¾")
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹é‡‹æ”¾å¤±æ•—ï¼š{e}")
        time.sleep(10)

threading.Thread(target=idle_monitor, daemon=True).start()

# è¼¸å…¥æ ¼å¼é©—è­‰
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: list[ChatMessage]
    max_tokens: int = 256
    temperature: float = 0.7
    stream: bool = False

# Gemma prompt æ ¼å¼
def build_prompt(messages):
    parts = []
    for m in messages:
        if m.role == "system":
            parts.append(f"<start_of_turn>system\n{m.content}<end_of_turn>")
        elif m.role == "user":
            parts.append(f"<start_of_turn>user\n{m.content}<end_of_turn>")
        elif m.role == "assistant":
            parts.append(f"<start_of_turn>model\n{m.content}<end_of_turn>")
    parts.append("<start_of_turn>model\n")
    return "\n".join(parts)

# SSE Streaming å›å‚³
async def stream_response(prompt, request_obj: ChatRequest, request: Request):
    created_ts = int(time.time())
    finish_reason = None
    llm_instance = get_llm()

    for chunk in llm_instance(prompt, max_tokens=request_obj.max_tokens, temperature=request_obj.temperature, stream=True):
        text = chunk["choices"][0]["text"]
        fr = chunk["choices"][0].get("finish_reason")
        if fr:
            finish_reason = fr

        if text:
            data = {
                "id": f"chatcmpl-{created_ts}",
                "object": "chat.completion.chunk",
                "model": request_obj.model,
                "choices": [{
                    "delta": {"content": text},
                    "index": 0,
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.01)

    final = {
        "id": f"chatcmpl-{created_ts}",
        "object": "chat.completion.chunk",
        "model": request_obj.model,
        "choices": [{
            "delta": {},
            "index": 0,
            "finish_reason": finish_reason or "stop"
        }]
    }
    yield f"data: {json.dumps(final, ensure_ascii=False)}\n\n"
    yield "data: [DONE]\n\n"

# API å…¥å£
@app.post("/v1/chat/completions")
async def chat(request: Request, body: ChatRequest):
    prompt = build_prompt(body.messages)
    if body.stream:
        return StreamingResponse(
            stream_response(prompt, body, request),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
    else:
        llm_instance = get_llm()
        output = llm_instance(prompt, max_tokens=body.max_tokens, temperature=body.temperature)
        return {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "model": body.model,
            "choices": [{
                "message": {
                    "role": "assistant",
                    "content": output["choices"][0]["text"].strip()
                },
                "finish_reason": output["choices"][0].get("finish_reason", "stop"),
                "index": 0
            }]
        }

# æœ¬åœ°å•Ÿå‹•ï¼ˆè«‹ç¢ºèªæª”æ¡ˆåç¨±ï¼‰
if __name__ == "__main__":
    uvicorn.run("server:app", host="0.0.0.0", port=8008, timeout_keep_alive=60)
